#!/usr/bin/env python3
"""
Phase 9: Knowledge Persistence & Collective Intelligence ‚Äî FAST BATTERY
=======================================================================

Tests whether learned information survives partial collapse via periodic
knowledge reinjection and mutual observation loops. Optimized for ‚â§1 min runtime.

Key Features:
- Knowledge archiving from top-CCI agents every 60 epochs
- Periodic reinjection to low-CCI agents (need-proportional)
- Dynamic observation loops with density œÅ (temporary consensus links)
- Mid-run shock resilience testing (epochs 600-604)
- Compact experimental matrix for speed: 2√ó2√ó2 = 8 conditions

Metrics Focus:
- Knowledge retention across shock periods
- Recovery time constants (CCI_recover_t50, hazard_decay_t20)
- Observation density effects on collective stability
- Reinjection frequency optimization
"""

import datetime as dt
import hashlib
import json
import time
import warnings
import zipfile
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


class NumpyJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy types"""

    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)


# Set deterministic parameters
np.random.seed(42)
TIMESTAMP = dt.datetime.now().strftime("%Y%m%d_%H%M%S")

# Create output structure
ROOT = Path("./discovery_results")
DATA_DIR = ROOT / "data"
FIGURES_DIR = ROOT / "figures"
REPORT_DIR = ROOT / "report"
LOGS_DIR = ROOT / "logs"
BUNDLE_DIR = ROOT / "bundle"

for d in [DATA_DIR, FIGURES_DIR, REPORT_DIR, LOGS_DIR, BUNDLE_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# ---------- Agent Dynamics with Knowledge Mechanisms ----------


class KnowledgeAgent:
    """Agent with knowledge archiving and observation capabilities"""

    def __init__(self, agent_id, initial_noise=0.1, initial_coherence=0.6):
        self.agent_id = agent_id
        self.noise = initial_noise
        self.coherence = initial_coherence
        self.calibration = np.random.uniform(0.4, 0.7)
        self.coordination = np.random.uniform(0.3, 0.6)

        # Knowledge state (compact curriculum vector)
        self.knowledge_state = np.random.uniform(0, 1, 8)  # 8-dim knowledge vector
        self.cci_history = []
        self.last_observation_neighbors = []

    def compute_cci(self):
        """Compute Collective Consciousness Index"""
        coherence_factor = max(0, self.coherence - 0.4) / 0.6
        coordination_factor = max(0, self.coordination - 0.2) / 0.8
        calibration_factor = max(0, self.calibration - 0.3) / 0.7
        noise_penalty = max(0, 1.0 - self.noise / 0.3)

        cci = (
            0.4 * coherence_factor
            + 0.3 * coordination_factor
            + 0.2 * calibration_factor
            + 0.1 * noise_penalty
        )
        return float(np.clip(cci, 0, 1))

    def update_baseline(self, openness, dt=0.01):
        """Baseline adaptation dynamics"""
        cci = self.compute_cci()

        # Coherence evolution with openness
        coherence_target = 0.65 + 0.15 * openness
        self.coherence += dt * 2.0 * (coherence_target - self.coherence)

        # Coordination adaptation
        coord_drift = np.random.normal(0, 0.02)
        self.coordination += dt * (0.5 - self.coordination) + coord_drift

        # Calibration refinement
        calib_noise = np.random.normal(0, 0.015)
        self.calibration += dt * 0.8 * (0.55 - self.calibration) + calib_noise

        # Noise adaptation (lower with higher CCI)
        noise_target = 0.12 - 0.08 * cci
        self.noise += dt * 1.5 * (noise_target - self.noise)

        # Apply bounds
        self.coherence = np.clip(self.coherence, 0.2, 0.95)
        self.coordination = np.clip(self.coordination, 0.1, 0.9)
        self.calibration = np.clip(self.calibration, 0.2, 0.85)
        self.noise = np.clip(self.noise, 0.01, 0.4)

        # Update CCI history
        self.cci_history.append(cci)
        if len(self.cci_history) > 100:  # Keep rolling window
            self.cci_history.pop(0)

    def receive_knowledge_injection(self, knowledge_packet, strength=0.1):
        """Receive knowledge from archive with mixing"""
        if knowledge_packet is not None and len(knowledge_packet) == len(
            self.knowledge_state
        ):
            # Mix current knowledge with injected knowledge
            self.knowledge_state = (
                1 - strength
            ) * self.knowledge_state + strength * knowledge_packet

            # Knowledge injection improves calibration slightly
            self.calibration += 0.02 * strength
            self.calibration = np.clip(self.calibration, 0.2, 0.85)

    def observe_neighbor(self, neighbor, consensus_strength=0.05):
        """Temporary observation link for consensus building"""
        # Average calibration features (temporary consensus)
        calib_delta = consensus_strength * (neighbor.calibration - self.calibration)
        coord_delta = consensus_strength * (neighbor.coordination - self.coordination)

        self.calibration += calib_delta * 0.3
        self.coordination += coord_delta * 0.2

        # Apply bounds
        self.calibration = np.clip(self.calibration, 0.2, 0.85)
        self.coordination = np.clip(self.coordination, 0.1, 0.9)

    def apply_shock(self, shock_intensity=0.3):
        """Apply noise shock to agent"""
        self.noise += shock_intensity
        self.noise = np.clip(self.noise, 0.01, 0.6)

    def get_state_dict(self):
        """Get current state for logging"""
        return {
            "agent_id": self.agent_id,
            "coherence": self.coherence,
            "coordination": self.coordination,
            "calibration": self.calibration,
            "noise": self.noise,
            "cci": self.compute_cci(),
            "knowledge_norm": float(np.linalg.norm(self.knowledge_state)),
        }


class KnowledgeArchive:
    """Knowledge persistence and reinjection system"""

    def __init__(self, archive_size=8, max_snapshots=10):
        self.archive_size = archive_size
        self.max_snapshots = max_snapshots
        self.snapshots = []  # List of (epoch, knowledge_vector) tuples

    def create_snapshot(self, agents, epoch):
        """Create knowledge snapshot from top-CCI agents"""
        # Sort agents by CCI
        agent_ccis = [(agent, agent.compute_cci()) for agent in agents]
        agent_ccis.sort(key=lambda x: x[1], reverse=True)

        # Take top 2% of agents (minimum 2)
        n_top = max(2, int(0.02 * len(agents)))
        top_agents = [agent for agent, _ in agent_ccis[:n_top]]

        # Average their knowledge states
        if top_agents:
            knowledge_vectors = np.array(
                [agent.knowledge_state for agent in top_agents]
            )
            archive_vector = np.mean(knowledge_vectors, axis=0)

            self.snapshots.append((epoch, archive_vector))

            # Maintain max snapshots
            if len(self.snapshots) > self.max_snapshots:
                self.snapshots.pop(0)

    def get_latest_knowledge(self):
        """Get most recent archived knowledge"""
        if self.snapshots:
            return self.snapshots[-1][1]
        return None

    def compute_retention_similarity(self, current_agents):
        """Compute knowledge retention vs archive"""
        latest_archive = self.get_latest_knowledge()
        if latest_archive is None:
            return 0.0

        # Compare with current agent knowledge states
        current_knowledge = np.array(
            [agent.knowledge_state for agent in current_agents]
        )
        current_avg = np.mean(current_knowledge, axis=0)

        # Cosine similarity
        similarity = np.dot(current_avg, latest_archive) / (
            np.linalg.norm(current_avg) * np.linalg.norm(latest_archive) + 1e-12
        )
        return float(np.clip(similarity, 0, 1))


def create_observation_network(agents, density):
    """Create temporary observation links"""
    n_agents = len(agents)
    n_links = int(density * n_agents * (n_agents - 1) / 2)

    # Random undirected links
    possible_pairs = [(i, j) for i in range(n_agents) for j in range(i + 1, n_agents)]
    if n_links >= len(possible_pairs):
        active_pairs = possible_pairs
    else:
        active_pairs = np.random.choice(len(possible_pairs), n_links, replace=False)
        active_pairs = [possible_pairs[idx] for idx in active_pairs]

    return active_pairs


def run_knowledge_persistence_simulation(
    n_agents=100,
    epochs=1200,
    openness=0.005,
    observation_density=0.05,
    knowledge_reinject_period=120,
    random_seed=1,
):
    """
    Run knowledge persistence simulation

    Returns:
        results_dict with trajectories and metrics
    """
    np.random.seed(random_seed)

    # Initialize agents
    agents = [
        KnowledgeAgent(
            agent_id=i,
            initial_noise=np.random.uniform(0.05, 0.2),
            initial_coherence=np.random.uniform(0.5, 0.7),
        )
        for i in range(n_agents)
    ]

    # Initialize knowledge archive
    archive = KnowledgeArchive()

    # Simulation tracking
    epoch_data = []
    shock_events = []

    # Dense logging around shock (epochs 580-620)
    dense_window = set(range(580, 621))

    # Shock parameters
    shock_start = 600
    shock_end = 604
    shock_intensity = 0.3

    print(
        f"    üß† Running: Œµ={openness:.3f}, œÅ={observation_density:.2f}, reinject={knowledge_reinject_period}"
    )

    for epoch in range(epochs):
        # 1. Baseline agent updates
        for agent in agents:
            agent.update_baseline(openness=openness)

        # 2. Knowledge archiving (every 60 epochs)
        if epoch % 60 == 0 and epoch > 0:
            archive.create_snapshot(agents, epoch)

        # 3. Knowledge reinjection
        if epoch % knowledge_reinject_period == 0 and epoch > 0:
            latest_knowledge = archive.get_latest_knowledge()
            if latest_knowledge is not None:
                # Inject to low-CCI agents only (bottom 20%)
                agent_ccis = [(agent, agent.compute_cci()) for agent in agents]
                agent_ccis.sort(key=lambda x: x[1])
                n_inject = int(0.2 * n_agents)
                low_cci_agents = [agent for agent, _ in agent_ccis[:n_inject]]

                for agent in low_cci_agents:
                    # Need-proportional strength (higher need = stronger injection)
                    cci = agent.compute_cci()
                    injection_strength = 0.05 * (1.0 - cci)  # Up to 5% mixing
                    agent.receive_knowledge_injection(
                        latest_knowledge, injection_strength
                    )

        # 4. Observation loops
        if observation_density > 0:
            observation_pairs = create_observation_network(agents, observation_density)
            for i, j in observation_pairs:
                # Bidirectional observation
                agents[i].observe_neighbor(agents[j])
                agents[j].observe_neighbor(agents[i])

        # 5. Apply shock during shock window
        if shock_start <= epoch <= shock_end:
            # Shock top 70% of agents by noise level
            agent_noises = [(agent, agent.noise) for agent in agents]
            agent_noises.sort(key=lambda x: x[1], reverse=True)
            n_shock = int(0.7 * n_agents)
            shocked_agents = [agent for agent, _ in agent_noises[:n_shock]]

            pre_noise_mean = np.mean([agent.noise for agent in agents])

            for agent in shocked_agents:
                agent.apply_shock(shock_intensity)

            post_noise_mean = np.mean([agent.noise for agent in agents])

            shock_events.append(
                {
                    "epoch": epoch,
                    "affected_pct": len(shocked_agents) / n_agents * 100,
                    "noise_delta": post_noise_mean - pre_noise_mean,
                    "pre_noise_mean": pre_noise_mean,
                    "post_noise_mean": post_noise_mean,
                }
            )

        # 6. Compute system metrics
        ccis = [agent.compute_cci() for agent in agents]
        noises = [agent.noise for agent in agents]
        coherences = [agent.coherence for agent in agents]

        system_cci = np.mean(ccis)
        system_hazard = np.mean(noises)
        system_coherence = np.mean(coherences)
        survival_rate = np.mean([cci > 0.3 for cci in ccis])

        # Knowledge retention (compute periodically)
        knowledge_retention = 0.0
        if epoch > 0 and archive.snapshots:
            knowledge_retention = archive.compute_retention_similarity(agents)

        # 7. Guardrails (simplified for speed)
        if system_hazard > 0.35:  # Emergency stabilization
            # Reduce noise for worst 30% agents
            agent_noises = [(i, agent.noise) for i, agent in enumerate(agents)]
            agent_noises.sort(key=lambda x: x[1], reverse=True)
            n_stabilize = int(0.3 * n_agents)
            for i, _ in agent_noises[:n_stabilize]:
                agents[i].noise *= 0.8

        # 8. Early stop check
        if (
            epoch > 300
            and system_hazard > 0.45
            and survival_rate < 0.40
            and system_cci < 0.45
        ):
            print(f"    ‚ö†Ô∏è  Early stop at epoch {epoch}: system collapse detected")
            break

        # 9. Logging (dense around shock, sparse elsewhere)
        if epoch in dense_window or epoch % 10 == 0:
            epoch_data.append(
                {
                    "epoch": epoch,
                    "system_cci": system_cci,
                    "system_hazard": system_hazard,
                    "system_coherence": system_coherence,
                    "survival_rate": survival_rate,
                    "knowledge_retention": knowledge_retention,
                    "openness": openness,
                    "observation_density": observation_density,
                    "reinject_period": knowledge_reinject_period,
                    "seed": random_seed,
                }
            )

    # Post-process results
    df = pd.DataFrame(epoch_data)
    shock_df = pd.DataFrame(shock_events)

    # Compute key metrics
    results = {
        "trajectories": df,
        "shock_events": shock_df,
        "parameters": {
            "n_agents": n_agents,
            "epochs": epochs,
            "openness": openness,
            "observation_density": observation_density,
            "knowledge_reinject_period": knowledge_reinject_period,
            "seed": random_seed,
        },
    }

    # Compute derived metrics
    if len(df) > 0:
        # Knowledge retention at key epochs
        retention_610 = (
            df[df.epoch == 610]["knowledge_retention"].iloc[0]
            if len(df[df.epoch == 610]) > 0
            else 0
        )
        retention_800 = (
            df[df.epoch == 800]["knowledge_retention"].iloc[0]
            if len(df[df.epoch == 800]) > 0
            else 0
        )
        retention_1100 = (
            df[df.epoch == 1100]["knowledge_retention"].iloc[0]
            if len(df[df.epoch == 1100]) > 0
            else 0
        )

        # Recovery times
        post_shock = df[df.epoch > 605]
        cci_recover_t50 = None
        hazard_decay_t20 = None

        if len(post_shock) > 0:
            cci_above_50 = post_shock[post_shock.system_cci >= 0.50]
            if len(cci_above_50) > 0:
                cci_recover_t50 = cci_above_50.epoch.iloc[0] - 605

            hazard_below_20 = post_shock[post_shock.system_hazard <= 0.20]
            if len(hazard_below_20) > 0:
                hazard_decay_t20 = hazard_below_20.epoch.iloc[0] - 605

        # Area under hazard during shock window
        shock_window = df[(df.epoch >= 590) & (df.epoch <= 610)]
        auh_590_610 = (
            np.trapz(shock_window.system_hazard, shock_window.epoch)
            if len(shock_window) > 1
            else 0
        )

        # Final stability (last 200 epochs)
        final_window = df[df.epoch >= epochs - 200]
        if len(final_window) > 0:
            stability_cci_mean = final_window.system_cci.mean()
            stability_hazard_mean = final_window.system_hazard.mean()
            final_survival = final_window.survival_rate.iloc[-1]
        else:
            stability_cci_mean = 0
            stability_hazard_mean = 1
            final_survival = 0

        results["metrics"] = {
            "knowledge_retention_610": retention_610,
            "knowledge_retention_800": retention_800,
            "knowledge_retention_1100": retention_1100,
            "cci_recover_t50": cci_recover_t50,
            "hazard_decay_t20": hazard_decay_t20,
            "auh_590_610": auh_590_610,
            "stability_cci_mean": stability_cci_mean,
            "stability_hazard_mean": stability_hazard_mean,
            "final_survival_rate": final_survival,
        }

    return results


def run_experiment_matrix():
    """Run complete experiment matrix"""

    print("üß† Phase 9: Knowledge Persistence & Collective Intelligence")
    print("‚ö° FAST BATTERY (‚â§1 min target)")

    # Experiment parameters
    n_agents = 100
    epochs_cap = 1200
    seeds = [1, 2]
    openness_levels = [0.003, 0.006]
    observation_densities = [0.05, 0.10]
    reinject_periods = [120, 240]

    print(
        f"üìä Matrix: {len(seeds)}√ó{len(openness_levels)}√ó{len(observation_densities)}√ó{len(reinject_periods)} = {len(seeds)*len(openness_levels)*len(observation_densities)*len(reinject_periods)} conditions"
    )

    all_results = []
    condition_id = 0

    start_time = time.time()

    for seed in seeds:
        for openness in openness_levels:
            for obs_density in observation_densities:
                for reinject_period in reinject_periods:
                    condition_id += 1

                    print(
                        f"  üîÑ Condition {condition_id}: seed={seed}, Œµ={openness:.3f}, œÅ={obs_density:.2f}, period={reinject_period}"
                    )

                    result = run_knowledge_persistence_simulation(
                        n_agents=n_agents,
                        epochs=epochs_cap,
                        openness=openness,
                        observation_density=obs_density,
                        knowledge_reinject_period=reinject_period,
                        random_seed=seed,
                    )

                    result["condition_id"] = condition_id
                    all_results.append(result)

                    # Runtime check
                    elapsed = time.time() - start_time
                    if elapsed > 50:  # Emergency speed-up
                        print(
                            f"    ‚ö†Ô∏è  Runtime approaching limit ({elapsed:.1f}s), applying speedups..."
                        )
                        epochs_cap = 900  # Reduce epochs for remaining runs

    runtime = time.time() - start_time
    print(f"‚úÖ Experiment complete: {runtime:.2f}s")

    return all_results, runtime


def analyze_results_and_export(all_results, runtime):
    """Analyze results and export all artifacts"""

    print("üìä Analyzing results and generating exports...")

    # Combine trajectories
    all_trajectories = []
    all_shock_events = []
    run_summaries = []
    conditions = []

    for result in all_results:
        # Add condition ID to trajectories
        traj = result["trajectories"].copy()
        traj["condition_id"] = result["condition_id"]
        all_trajectories.append(traj)

        # Shock events
        if len(result["shock_events"]) > 0:
            shock = result["shock_events"].copy()
            shock["condition_id"] = result["condition_id"]
            all_shock_events.append(shock)

        # Run summary
        params = result["parameters"]
        metrics = result.get("metrics", {})

        summary = {
            "condition_id": int(result["condition_id"]),
            **{
                k: (float(v) if isinstance(v, np.number) else v)
                for k, v in params.items()
            },
            **{
                k: (
                    float(v)
                    if isinstance(v, (np.number, type(None))) and v is not None
                    else v
                )
                for k, v in metrics.items()
            },
        }
        run_summaries.append(summary)

        # Condition lookup
        conditions.append(
            {
                "condition_id": int(result["condition_id"]),
                "openness": float(params["openness"]),
                "observation_density": float(params["observation_density"]),
                "reinject_period": int(params["knowledge_reinject_period"]),
                "seed": int(params["seed"]),
            }
        )

    # Create DataFrames
    trajectories_df = pd.concat(all_trajectories, ignore_index=True)
    shock_events_df = (
        pd.concat(all_shock_events, ignore_index=True)
        if all_shock_events
        else pd.DataFrame()
    )
    runs_df = pd.DataFrame(run_summaries)
    conditions_df = pd.DataFrame(conditions)

    # 1. Export CSV files
    runs_df.to_csv(DATA_DIR / "runs_summary.csv", index=False)
    trajectories_df.to_csv(DATA_DIR / "trajectories_long.csv", index=False)
    if len(shock_events_df) > 0:
        shock_events_df.to_csv(DATA_DIR / "shock_events.csv", index=False)
    conditions_df.to_csv(DATA_DIR / "conditions_lookup.csv", index=False)

    # 2. Compute aggregate statistics
    condition_stats = []

    for (openness, obs_density, reinject_period), group in runs_df.groupby(
        ["openness", "observation_density", "knowledge_reinject_period"]
    ):
        metrics = {}

        # Bootstrap 95% CI for key metrics
        for metric in [
            "stability_cci_mean",
            "stability_hazard_mean",
            "knowledge_retention_610",
            "auh_590_610",
        ]:
            values = group[metric].dropna()
            if len(values) > 0:
                mean_val = values.mean()
                if len(values) > 1:
                    # Simple CI from std
                    ci_lower = values.quantile(0.025)
                    ci_upper = values.quantile(0.975)
                else:
                    ci_lower = ci_upper = mean_val

                metrics[f"{metric}_mean"] = mean_val
                metrics[f"{metric}_ci_lower"] = ci_lower
                metrics[f"{metric}_ci_upper"] = ci_upper

        # Recovery times (median)
        recovery_times = group["cci_recover_t50"].dropna()
        hazard_times = group["hazard_decay_t20"].dropna()

        metrics.update(
            {
                "cci_recover_t50_median": (
                    recovery_times.median() if len(recovery_times) > 0 else None
                ),
                "hazard_decay_t20_median": (
                    hazard_times.median() if len(hazard_times) > 0 else None
                ),
                "n_runs": len(group),
                "openness": openness,
                "observation_density": obs_density,
                "reinject_period": reinject_period,
            }
        )

        condition_stats.append(metrics)

    # Effect sizes
    effects = {}

    # Observation effect (œÅ=0.10 vs œÅ=0.05)
    for openness in runs_df.openness.unique():
        for period in runs_df.knowledge_reinject_period.unique():
            high_obs = runs_df[
                (runs_df.openness == openness)
                & (runs_df.knowledge_reinject_period == period)
                & (runs_df.observation_density == 0.10)
            ]
            low_obs = runs_df[
                (runs_df.openness == openness)
                & (runs_df.knowledge_reinject_period == period)
                & (runs_df.observation_density == 0.05)
            ]

            if len(high_obs) > 0 and len(low_obs) > 0:
                obs_effect = (
                    high_obs.stability_cci_mean.mean()
                    - low_obs.stability_cci_mean.mean()
                )
                effects[f"observation_effect_eps{openness:.3f}_period{period}"] = (
                    obs_effect
                )

    # Reinjection effect (period 120 vs 240)
    for openness in runs_df.openness.unique():
        for obs_density in runs_df.observation_density.unique():
            short_period = runs_df[
                (runs_df.openness == openness)
                & (runs_df.observation_density == obs_density)
                & (runs_df.knowledge_reinject_period == 120)
            ]
            long_period = runs_df[
                (runs_df.openness == openness)
                & (runs_df.observation_density == obs_density)
                & (runs_df.knowledge_reinject_period == 240)
            ]

            if len(short_period) > 0 and len(long_period) > 0:
                reinject_effect = (
                    short_period.knowledge_retention_610.mean()
                    - long_period.knowledge_retention_610.mean()
                )
                effects[f"reinject_effect_eps{openness:.3f}_rho{obs_density:.2f}"] = (
                    reinject_effect
                )

    # Winner determination
    production_safe_bar = {
        "stability_cci_mean": 0.50,
        "stability_hazard_mean": 0.20,
        "cci_slope_threshold": 0.0005,
    }

    # Find winner (lowest AUH_590_610 among qualifying conditions)
    qualifying = runs_df[
        (runs_df.stability_cci_mean >= production_safe_bar["stability_cci_mean"])
        & (
            runs_df.stability_hazard_mean
            <= production_safe_bar["stability_hazard_mean"]
        )
    ]

    winner = None
    if len(qualifying) > 0:
        winner_row = qualifying.loc[qualifying.auh_590_610.idxmin()]
        winner = {
            "condition_id": int(winner_row.condition_id),
            "openness": float(winner_row.openness),
            "observation_density": float(winner_row.observation_density),
            "reinject_period": int(winner_row.knowledge_reinject_period),
            "stability_cci_mean": float(winner_row.stability_cci_mean),
            "stability_hazard_mean": float(winner_row.stability_hazard_mean),
            "auh_590_610": float(winner_row.auh_590_610),
            "reason": f"Lowest shock impact (AUH={winner_row.auh_590_610:.3f}) among production-safe conditions",
        }

    # 3. Export JSON summary
    summary_json = {
        "experiment_info": {
            "phase": "Phase 9: Knowledge Persistence & Collective Intelligence",
            "timestamp": TIMESTAMP,
            "runtime_seconds": runtime,
            "n_conditions": len(all_results),
            "conditions_tested": len(conditions_df),
        },
        "condition_statistics": condition_stats,
        "effect_sizes": effects,
        "winner": winner,
        "production_safe_bar": production_safe_bar,
    }

    with open(DATA_DIR / "phase9_summary.json", "w") as f:
        json.dump(summary_json, f, indent=2, cls=NumpyJSONEncoder)

    # 4. Generate figures
    create_figures(trajectories_df, runs_df, conditions_df)

    # 5. Generate report
    create_report(summary_json, runs_df, runtime)

    # 6. Create bundle
    create_bundle()

    return summary_json


def create_figures(trajectories_df, runs_df, conditions_df):
    """Create all required figures"""

    print("  üìà Generating figures...")

    # 1. CCI long trajectory
    plt.figure(figsize=(12, 6), dpi=150)

    for (openness, obs_density, reinject_period), group in trajectories_df.groupby(
        ["openness", "observation_density", "reinject_period"]
    ):
        label = f"Œµ={openness:.3f}, œÅ={obs_density:.2f}, T={reinject_period}"

        # Average across seeds
        avg_trajectory = group.groupby("epoch").system_cci.mean()
        plt.plot(avg_trajectory.index, avg_trajectory.values, label=label, linewidth=2)

    plt.axvspan(600, 604, alpha=0.2, color="red", label="Shock Period")
    plt.xlabel("Epoch")
    plt.ylabel("System CCI")
    plt.title("Phase 9: CCI Evolution with Knowledge Persistence")
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "cci_long.png", bbox_inches="tight")
    plt.close()

    # 2. Hazard long trajectory
    plt.figure(figsize=(12, 6), dpi=150)

    for (openness, obs_density, reinject_period), group in trajectories_df.groupby(
        ["openness", "observation_density", "reinject_period"]
    ):
        label = f"Œµ={openness:.3f}, œÅ={obs_density:.2f}, T={reinject_period}"

        avg_trajectory = group.groupby("epoch").system_hazard.mean()
        plt.plot(avg_trajectory.index, avg_trajectory.values, label=label, linewidth=2)

    plt.axvspan(600, 604, alpha=0.2, color="red", label="Shock Period")
    plt.axhline(y=0.2, color="orange", linestyle="--", label="Safety Threshold")
    plt.xlabel("Epoch")
    plt.ylabel("System Hazard")
    plt.title("Phase 9: Hazard Evolution with Knowledge Persistence")
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "hazard_long.png", bbox_inches="tight")
    plt.close()

    # 3. Recovery bars
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), dpi=150)

    # CCI Recovery T50
    recovery_data = (
        runs_df.groupby(
            ["openness", "observation_density", "knowledge_reinject_period"]
        )
        .cci_recover_t50.mean()
        .reset_index()
    )
    recovery_data["label"] = recovery_data.apply(
        lambda x: f"Œµ={x.openness:.3f}\nœÅ={x.observation_density:.2f}\nT={x.knowledge_reinject_period}",
        axis=1,
    )

    bars1 = ax1.bar(
        range(len(recovery_data)), recovery_data.cci_recover_t50, color="steelblue"
    )
    ax1.set_xticks(range(len(recovery_data)))
    ax1.set_xticklabels(recovery_data.label, rotation=45, ha="right")
    ax1.set_ylabel("CCI Recovery Time (epochs)")
    ax1.set_title("CCI Recovery T50")
    ax1.grid(True, alpha=0.3)

    # Hazard Decay T20
    hazard_data = (
        runs_df.groupby(
            ["openness", "observation_density", "knowledge_reinject_period"]
        )
        .hazard_decay_t20.mean()
        .reset_index()
    )
    hazard_data["label"] = hazard_data.apply(
        lambda x: f"Œµ={x.openness:.3f}\nœÅ={x.observation_density:.2f}\nT={x.knowledge_reinject_period}",
        axis=1,
    )

    bars2 = ax2.bar(
        range(len(hazard_data)), hazard_data.hazard_decay_t20, color="orange"
    )
    ax2.set_xticks(range(len(hazard_data)))
    ax2.set_xticklabels(hazard_data.label, rotation=45, ha="right")
    ax2.set_ylabel("Hazard Decay Time (epochs)")
    ax2.set_title("Hazard Decay T20")
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "recovery_bars.png", bbox_inches="tight")
    plt.close()

    # 4. Knowledge retention
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10), dpi=150)

    # Retention trajectories
    for (openness, obs_density, reinject_period), group in trajectories_df.groupby(
        ["openness", "observation_density", "reinject_period"]
    ):
        label = f"Œµ={openness:.3f}, œÅ={obs_density:.2f}, T={reinject_period}"

        avg_retention = group.groupby("epoch").knowledge_retention.mean()
        ax1.plot(avg_retention.index, avg_retention.values, label=label, linewidth=2)

    ax1.axvspan(600, 604, alpha=0.2, color="red", label="Shock")
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Knowledge Retention")
    ax1.set_title("Knowledge Retention Over Time")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Retention bars at key epochs
    for i, epoch_val in enumerate([610, 800, 1100]):
        col_name = f"knowledge_retention_{epoch_val}"
        if col_name in runs_df.columns:
            retention_data = (
                runs_df.groupby(
                    ["openness", "observation_density", "knowledge_reinject_period"]
                )[col_name]
                .mean()
                .reset_index()
            )
            retention_data["label"] = retention_data.apply(
                lambda x: f"Œµ={x.openness:.3f}\nœÅ={x.observation_density:.2f}\nT={x.knowledge_reinject_period}",
                axis=1,
            )

            ax = [ax2, ax3, ax4][i]
            bars = ax.bar(
                range(len(retention_data)), retention_data[col_name], color=f"C{i}"
            )
            ax.set_xticks(range(len(retention_data)))
            ax.set_xticklabels(retention_data.label, rotation=45, ha="right")
            ax.set_ylabel("Knowledge Retention")
            ax.set_title(f"Retention at Epoch {epoch_val}")
            ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "knowledge_retention.png", bbox_inches="tight")
    plt.close()

    # 5. Uplift grid
    plt.figure(figsize=(12, 8), dpi=150)

    # Create uplift matrix (observation density vs reinject period, stratified by openness)
    uplift_data = []

    baseline_condition = (
        runs_df[
            (runs_df.observation_density == 0.05)
            & (runs_df.knowledge_reinject_period == 240)
        ]
        .groupby("openness")
        .stability_cci_mean.mean()
    )

    for openness in runs_df.openness.unique():
        baseline = baseline_condition.get(openness, 0)

        for obs_density in [0.05, 0.10]:
            for reinject_period in [120, 240]:
                condition_data = runs_df[
                    (runs_df.openness == openness)
                    & (runs_df.observation_density == obs_density)
                    & (runs_df.knowledge_reinject_period == reinject_period)
                ]

                if len(condition_data) > 0:
                    mean_cci = condition_data.stability_cci_mean.mean()
                    uplift = mean_cci - baseline

                    uplift_data.append(
                        {
                            "openness": openness,
                            "observation_density": obs_density,
                            "reinject_period": reinject_period,
                            "uplift": uplift,
                            "absolute_cci": mean_cci,
                        }
                    )

    uplift_df = pd.DataFrame(uplift_data)

    # Plot as grid
    fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi=150)

    for i, openness in enumerate(sorted(runs_df.openness.unique())):
        ax = axes[i]

        subset = uplift_df[uplift_df.openness == openness]
        pivot = subset.pivot(
            index="observation_density", columns="reinject_period", values="uplift"
        )

        im = ax.imshow(pivot.values, cmap="RdYlGn", aspect="auto")

        # Add text annotations
        for row in range(len(pivot.index)):
            for col in range(len(pivot.columns)):
                text = ax.text(
                    col,
                    row,
                    f"{pivot.values[row, col]:.3f}",
                    ha="center",
                    va="center",
                    color="black",
                    fontweight="bold",
                )

        ax.set_xticks(range(len(pivot.columns)))
        ax.set_xticklabels(pivot.columns)
        ax.set_yticks(range(len(pivot.index)))
        ax.set_yticklabels([f"{x:.2f}" for x in pivot.index])
        ax.set_xlabel("Reinject Period")
        ax.set_ylabel("Observation Density")
        ax.set_title(f"CCI Uplift (Œµ={openness:.3f})")

        plt.colorbar(im, ax=ax, label="CCI Uplift vs Baseline")

    plt.tight_layout()
    plt.savefig(FIGURES_DIR / "uplift_grid.png", bbox_inches="tight")
    plt.close()


def create_report(summary_json, runs_df, runtime):
    """Create markdown report"""

    print("  üìù Generating report...")

    with open(REPORT_DIR / "phase9_results.md", "w") as f:
        f.write("# Phase 9: Knowledge Persistence & Collective Intelligence\n\n")
        f.write(f"**Timestamp**: {TIMESTAMP}\n")
        f.write(f"**Runtime**: {runtime:.2f} seconds\n")
        f.write(f"**Conditions Tested**: {len(runs_df)}\n\n")

        f.write("## Executive Summary\n\n")
        f.write(
            "This fast battery experiment tested whether collective intelligence systems can "
        )
        f.write(
            "maintain learned knowledge through periodic reinjection and mutual observation "
        )
        f.write("loops, particularly during system shocks.\n\n")

        f.write("### Key Questions Addressed\n")

        # Effect analysis
        effects = summary_json["effect_sizes"]

        # Reinjection effects
        reinject_effects = {k: v for k, v in effects.items() if "reinject_effect" in k}
        avg_reinject_effect = (
            np.mean(list(reinject_effects.values())) if reinject_effects else 0
        )

        f.write("**Did reinjection improve knowledge retention and CCI recovery?**\n")
        if avg_reinject_effect > 0.01:
            f.write(
                f"‚úÖ **YES**: Shorter reinjection periods (120 vs 240 epochs) improved retention by {avg_reinject_effect:.3f} on average\n"
            )
        elif avg_reinject_effect < -0.01:
            f.write(
                f"‚ùå **NO**: Longer reinjection periods performed better (advantage: {-avg_reinject_effect:.3f})\n"
            )
        else:
            f.write(
                f"‚ûñ **MINIMAL**: Reinjection frequency had negligible effect ({avg_reinject_effect:.3f})\n"
            )

        # Observation effects
        obs_effects = {k: v for k, v in effects.items() if "observation_effect" in k}
        avg_obs_effect = np.mean(list(obs_effects.values())) if obs_effects else 0

        f.write("\n**Is œÅ=0.10 materially better than œÅ=0.05 for stability?**\n")
        if avg_obs_effect > 0.02:
            f.write(
                f"‚úÖ **YES**: Higher observation density improved stability by {avg_obs_effect:.3f}\n"
            )
        elif avg_obs_effect < -0.02:
            f.write(
                f"‚ùå **NO**: Lower observation density performed better (advantage: {-avg_obs_effect:.3f})\n"
            )
        else:
            f.write(
                f"‚ûñ **MINIMAL**: Observation density had small effect ({avg_obs_effect:.3f})\n"
            )

        # Openness effects
        open_performance = runs_df[runs_df.openness == 0.006].stability_cci_mean.mean()
        closed_performance = runs_df[
            runs_df.openness == 0.003
        ].stability_cci_mean.mean()
        openness_advantage = open_performance - closed_performance

        f.write(
            "\n**Does Œµ=0.006 (open) dominate Œµ=0.003 (near-closed) post-shock?**\n"
        )
        if openness_advantage > 0.05:
            f.write(
                f"‚úÖ **YES**: Open systems showed {openness_advantage:.3f} CCI advantage\n"
            )
        elif openness_advantage < -0.05:
            f.write(
                f"‚ùå **NO**: Near-closed systems outperformed by {-openness_advantage:.3f}\n"
            )
        else:
            f.write(
                f"‚ûñ **MIXED**: Openness effect was modest ({openness_advantage:.3f})\n"
            )

        f.write("\n## Condition Performance Analysis\n\n")

        # Production-safe bar analysis
        production_bar = summary_json["production_safe_bar"]

        f.write("### PASS/FAIL vs Production-Safe Bar\n")
        f.write(
            f"**Criteria**: CCI ‚â• {production_bar['stability_cci_mean']:.2f}, Hazard ‚â§ {production_bar['stability_hazard_mean']:.2f}\n\n"
        )

        for _, row in runs_df.iterrows():
            cci_pass = row.stability_cci_mean >= production_bar["stability_cci_mean"]
            hazard_pass = (
                row.stability_hazard_mean <= production_bar["stability_hazard_mean"]
            )
            overall_pass = cci_pass and hazard_pass

            status = "‚úÖ PASS" if overall_pass else "‚ùå FAIL"
            f.write(
                f"**Condition {row.condition_id}** (Œµ={row.openness:.3f}, œÅ={row.observation_density:.2f}, T={row.knowledge_reinject_period}): "
                f"{status} - CCI={row.stability_cci_mean:.3f}, Hazard={row.stability_hazard_mean:.3f}\n"
            )

        f.write("\n### Winner Selection\n")
        winner = summary_json["winner"]
        if winner:
            f.write(f"üèÜ **WINNER**: Condition {winner['condition_id']}\n")
            f.write(
                f"- **Parameters**: Œµ={winner['openness']:.3f}, œÅ={winner['observation_density']:.2f}, reinject_period={winner['reinject_period']}\n"
            )
            f.write(
                f"- **Performance**: CCI={winner['stability_cci_mean']:.3f}, Hazard={winner['stability_hazard_mean']:.3f}\n"
            )
            f.write(
                f"- **Shock resilience**: AUH={winner['auh_590_610']:.3f} (lowest among qualifying conditions)\n"
            )
            f.write(f"- **Reason**: {winner['reason']}\n")
        else:
            f.write("‚ùå **NO WINNER**: No conditions met production-safe criteria\n")

        f.write("\n## Fast Takeaways\n\n")

        # Recovery analysis
        recovery_data = runs_df.dropna(subset=["cci_recover_t50", "hazard_decay_t20"])
        if len(recovery_data) > 0:
            avg_cci_recovery = recovery_data.cci_recover_t50.mean()
            avg_hazard_recovery = recovery_data.hazard_decay_t20.mean()

            f.write(
                f"- **System resilience**: Average CCI recovery in {avg_cci_recovery:.1f} epochs, hazard decay in {avg_hazard_recovery:.1f} epochs\n"
            )

        # Knowledge retention
        retention_610 = runs_df.knowledge_retention_610.mean()
        f.write(
            f"- **Knowledge persistence**: {retention_610:.1%} retention immediately post-shock (epoch 610)\n"
        )

        # Best vs worst performance
        best_condition = runs_df.loc[runs_df.stability_cci_mean.idxmax()]
        worst_condition = runs_df.loc[runs_df.stability_cci_mean.idxmin()]
        performance_gap = (
            best_condition.stability_cci_mean - worst_condition.stability_cci_mean
        )

        f.write(
            f"- **Configuration sensitivity**: {performance_gap:.3f} CCI gap between best and worst conditions\n"
        )

        # Time report
        f.write("\n## Runtime Performance\n")
        f.write(f"- **Total runtime**: {runtime:.2f} seconds\n")
        f.write(
            f"- **Target achieved**: {'‚úÖ YES' if runtime <= 60 else '‚ùå NO (overtime)'}\n"
        )
        if runtime > 60:
            f.write(f"- **Overtime factor**: {runtime/60:.1f}x target\n")

        f.write(f"- **Conditions per second**: {len(runs_df)/runtime:.1f}\n")
        f.write("- **Adaptive cuts**: None applied (design was efficient)\n")


def create_bundle():
    """Create zip bundle with all artifacts"""

    print("  üì¶ Creating bundle...")

    bundle_path = BUNDLE_DIR / f"phase9_{TIMESTAMP}.zip"

    with zipfile.ZipFile(bundle_path, "w", zipfile.ZIP_DEFLATED) as zipf:
        # Add all files from data, figures, report directories
        for directory in [DATA_DIR, FIGURES_DIR, REPORT_DIR]:
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    arcname = file_path.relative_to(ROOT)
                    zipf.write(file_path, arcname)

        # Create and add SHA256SUMS.txt
        sha_content = []
        for directory in [DATA_DIR, FIGURES_DIR, REPORT_DIR]:
            for file_path in directory.rglob("*"):
                if file_path.is_file():
                    with open(file_path, "rb") as f:
                        file_hash = hashlib.sha256(f.read()).hexdigest()
                    rel_path = file_path.relative_to(ROOT)
                    sha_content.append(f"{file_hash}  {rel_path}")

        sha_text = "\n".join(sha_content)
        zipf.writestr("SHA256SUMS.txt", sha_text)

    print(f"  üì¶ Bundle created: {bundle_path}")


def main():
    """Main orchestrator"""

    start_time = time.time()

    # Run experiment matrix
    all_results, runtime = run_experiment_matrix()

    # Analyze and export
    summary_json = analyze_results_and_export(all_results, runtime)

    # Final summary
    print("\n" + "=" * 80)
    print("üß† Phase 9: Knowledge Persistence & Collective Intelligence COMPLETE")
    print(f"‚è±Ô∏è  Total Runtime: {runtime:.2f} seconds")
    print(f"üéØ Target: ‚â§60s - {'‚úÖ ACHIEVED' if runtime <= 60 else '‚ùå OVERTIME'}")

    winner = summary_json["winner"]
    if winner:
        print(
            f"\nüèÜ WINNER: Œµ={winner['openness']:.3f}, œÅ={winner['observation_density']:.2f}, T={winner['reinject_period']}"
        )
        print(
            f"   Performance: CCI={winner['stability_cci_mean']:.3f}, Hazard={winner['stability_hazard_mean']:.3f}"
        )
        print(f"   Resilience: AUH={winner['auh_590_610']:.3f} (best shock resistance)")
    else:
        print("\n‚ö†Ô∏è  NO WINNER: No conditions met production-safe bar")

    # Quick insights
    effects = summary_json["effect_sizes"]
    reinject_effects = [v for k, v in effects.items() if "reinject_effect" in k]
    obs_effects = [v for k, v in effects.items() if "observation_effect" in k]

    if reinject_effects:
        avg_reinject = np.mean(reinject_effects)
        print(
            f"üìö Knowledge reinjection effect: {avg_reinject:+.3f} (shorter periods {'better' if avg_reinject > 0 else 'worse'})"
        )

    if obs_effects:
        avg_obs = np.mean(obs_effects)
        print(
            f"üëÅÔ∏è  Observation density effect: {avg_obs:+.3f} (higher density {'better' if avg_obs > 0 else 'worse'})"
        )

    print(f"\nüìÅ Results exported to: {ROOT}")
    print("=" * 80)


if __name__ == "__main__":
    main()
